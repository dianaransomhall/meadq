%\VignetteIndexEntry{meadq }
%\VignetteKeywords{MEA}
%\VignettePackage{meadq}
%\VignetteEngine{knitr::knitr}
\documentclass{article}
\usepackage{mathpazo}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[a4paper,left=2cm,right=4cm,top=2cm,bottom=2cm]{geometry}
\usepackage{setspace}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{graphicx}

\usepackage{xspace,amsmath}
\newcommand{\um}{\ensuremath{\mu \text{m}}\xspace}
\usepackage{url}
\usepackage[authoryear]{natbib}
\newcommand{\dynamic}{(Dynamic)}
\newcommand{\static}{(Static)}
\newcommand{\hdfgroup}[1]{\texttt{#1}}


\begin{document}

\onehalfspacing
\title{meadq package: from A to Z}

\author{Diana Hall}
\date{\today}

\maketitle

\section*{Introduction}

This is an introduction to the abilities of the meadq package
for analysis of multielectrode array data.  The program is specifically suited to analyze data from the 12 \& 48 well MEA plates, such as those from Axion Biosystems.  It is not a comprehensive guide, but simply gives examples of what can be done with the package.
The package contains some example data sets which are used here to
demonstrate various routines. 

% * is used to remove the default numbering of sections
\section*{Installation}
To install this package, a binary source code may be used or github may be used for latest version. Package sources via the URL \url{http://github.com/dianaransomhall/meadq}.and then view this introductory vignette

\subsection*{Installation using source}
<<install-sjemea, eval=FALSE, echo=TRUE>>=
tarfile.path = "F:/R/Rpackage_meadq/meadq_1.0.1.tar.gz"
install.packages(pkgs = tarfile.path, type="source", repos = NULL)
vignette('meadq-intro', package='meadq')
@

\subsection*{Installation using github }
Alternatively, most recent version meadq may be downloaded from github. ``devtools'' R-package will need to be installed to enable the install\textunderscore github functionality. 
<<install-sjemea-github,eval=FALSE,echo=TRUE>>=
install.package("devtools") # note: need R v.>=3.1.0
# to install devtools, use CRAN
require(devtools)
install_github("dianaransomhall/meadq")
@ 


\section{R software reference}
For those new to R, the free software may be downloaded from \url{http://www.cran.r-project.org}.  A highly user-friendly interface may be downloaded here \url{http://www.rstudio.com} with a guide here \url{http://www.dss.princeton.edu/training/RStudio101.pdf}.  A beginners guide to R is available here \url{http://www.cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf} and inevitable trouble shooting is best done by googling, as web documentation on R abounds.

\subsection*{Vignette}
To view the introductory vingette:
<<vignette, eval=FALSE, echo=TRUE>>=
vignette('meadq-intro', package='meadq')
@

\subsection*{Setup}
Install and load packages into current R session.
<<setup, message=FALSE, echo=TRUE, eval=F>>=
install_github("sje30/sjemea") # companion pkg to meadq
install_github("yihui/knitr") #to compile vignette
source("http://bioconductor.org/biocLite.R") #for HDF5 files
biocLite("rhdf5")
#load packages into current session
require(rhdf5)
@
This file is a vignette, written in R, as a reproducible research document.
<<vignette-setup, message=FALSE, echo=TRUE, eval=TRUE>>=
require(meadq)
require(knitr)
opts_chunk$set(cache=TRUE) #to save time if results previously generated
opts_chunk$set(dev='pdf')
@

\subsection*{Help pages}
A list of help pages associated with the package is given by:
<<help-pages, eval=FALSE, message=FALSE>>=
help(package='meadq') 
@




\section*{File naming conventions and experiment log file } 
File names help store meta-data about file as well as provide a unique identifying used in the link to the experiment log file.  The experiment log file serves as more in depth documentation of what was done in experment.  Together the log file and the file name provide redundancy in experimental protocol useful as a check and also make data sets readily usable for those unfamiliar with data.

\subsection*{File naming conventions}
Aspects of the file naming conventions must be followed in order that the code executes without error. Those aspects which much be followed are that the first 4 chunks must be seperated by ``\textunderscore '' and must give meta-data that matches with the experimental log file .csv file. This is necessary for meadq to match the input file to experimental meta-data in the log file and combine them in the resulting hdf5 file. 
\\*Let's use the example file provdied within the package subdirectory``extdata'': 

\begin{description}
\item[spike train text file]
ON\textunderscore 20140205\textunderscore MW1007-26\textunderscore DIV07\textunderscore 001.mapTimestamps 
\item[log file] Experiment\textunderscore LogFileExample.csv
\end{description}


\begin{description}

\item[Project] ``ON'' codes the project or group of experiments: ``ON'' for ontogeny. 
\begin{description}
\item[position in file name] immediately preceeding the first `\textunderscore '
\item[format] The project desination may be of any length or capitalization but needs to begin with a letter.
\item[designation in log file] The corresponding column in the log file must be entitled ``Project''. The contents of the ``Project'' column in the log file and the project designation in file name must be identical.
\end{description}


\item[Experiment Date] ``20140205'' denotes the date of the plating of the neurons, February 5, 2014. 
\begin{description}
\item[position in file name] immediate following the first `\textunderscore '
\item[format] YYYYMMDD 
\item[designation in log file] The corresponding column in the log file must be entitled ``Experiment Date''. The contents of the ``Experiment Date'' column in log file must be identical to the Experiment Date designation in file name.
\end{description}


\item[Plate SN ]  ``MW1007-26'' is the serial number (SN) of the plate used in the recording.  
\begin{description}
\item[position in file name] The serial number always follows the second `\textunderscore '.
\item[format] any format alpha or numeric, so long as serial number contains no  \textunderscore
\item[designation in log file] The corresponding column in the log file must be entitled ``Plate SN''. The contents of column entitled ``Plate SN'' in the log file must be identical to the plate serial number designation in the file name.
\end{description}

\item[DIV] ``DIV07'' denotes the 7th day in vitro (DIV) of the plated cells.
\begin{description}
\item[position in file name] After the 3rd ` \textunderscore '.
\item[format]  ``DIV07'', ``DIV7'', ``07'', ``7'' are all permissible so long as a exact match is made to log file.
\item[designation in log file] The DIV column in the log file must be entitled ``DIV''. The contents of the ``DIV'' column in the log file and the DIV designation in the file name must be identical.
\end{description}

\item[Users Choice] Any other naming conventions may be added to end of file name. Example data has `001' to indicate sequential order of recording should more than one be made with same meta-data.  Other examples may include indication of `pre' or  `post'  should there be pre and pose dose recordings.

\end{description}




\subsection*{Experiment log file conventions}
Experiment log file must be a .csv (comma seperated value) file that contain the names and information listed below.  See  ``Experiment\textunderscore LogFileExample.csv'' located in the ``extdata'' subdirectory of package for an example.

Column names in log file must match verbatim to list below (case sensitive):
\begin{description}
\item[Project] \mbox{}\\
\begin{description}
\item[format] Must match file name.  See ``Project'' in above subsection.
\end{description}

\item[Experiment Date] \mbox{}\\
\begin{description}
\item[format] Must match file name. See ``Experiment Date'' in above subsection.  
\end{description}

\item[Plate SN] \mbox{}\\
\begin{description}
\item[format] Must match file name. See ``Plate SN'' in above subsection.  
\end{description}


\item[DIV] \mbox{}\\
\begin{description}
\item[format] Must correspond to file name. See ``DIV'' in above subsection. 
\end{description}


\item[Well] \mbox{}\\
\begin{description}
\item[format] one row in log file for each well; must be a capital letter preceeded by a number e.g. "A1"or "F3" 
\end{description}

\item[Treatment] \mbox{}\\
\begin{description}
\item[format] any character string without spaces is permissible.
\end{description}

\item[Size] \mbox{}\\
\begin{description}
\item[format] give any size data about anything or list NA in column if no size specification exist 
\end{description}

\item[Dose] \mbox{}\\
\begin{description}
\item[format] dose of treatment. Any rational, or integer number is permissible 
\end{description}

\item[Units] \mbox{}\\
\begin{description}
\item[format] any-alpha numeric unit specification is permissible 
\end{description}

\end{description}






\section*{Creating HDF5 Files}
Converting data into HDF5 file format is beneficial for many reasons. Firstly, converting the recorded data from a proprietary or otherwise restrictive file type into a universal file type, such as HDF5, facilitates data sharing and provides flexibility in analysis software choice.  Secondly, the ``program on standards for datasharing'' by the International Neuroinformatics Coordinating Facility \url{ http://www.datasharing.incf.ord/ep/HDF5_data_standard} has recommended the HDF5 format. Finally, HDF5 files are designed to store meta-data and data seperately for maximum efficiency. For more information of the HDF5 format see \url{http://www.hdfgroup.org/HDF5/doc/H5.into.html}.
 Code is provided to convert axion alpha map files (.map extension) to text files and finally to HDF5 (.h5 extention) format files and in the process store meta-data inside recording file. The intermediate step of converting alpha map files to text files is necessary since alpha map files are in binary and may only be decoded by a third party software such as NeuroExplorer \citep{neuroexplorer}.

\subsection{create text files from alpha map files} \mbox{}\\
NeuroExplorer has a scripting language that may be used to batch create text files (.mapTimestamps extension) of spike trains from apha map files.  meadq needs text spike trains to use the ``.mapTimestamps'' file extension.  For information regarding script language see manual
here \url{http://www.neuroexplorer.com/downloads/Nex3Manual.pdf}.  A neuroExplorer script entitled ``Export\textunderscore timestamps\textunderscore from\textunderscore map\textunderscore files\textunderscore meadqExample.nsc'' is provided in the ``extdata'' subdirectory of
meadq folder. 

\subsection*{Execution}
\begin{description}
\item[interactive session] The script may be executed by initiating an interactive session of NeuroExplorer, opening the script and clicking the green play button.  A dialogue box will appear in the NeuroExplorer main page prompting for user to enter directory containing .map files to be converted.  Enter the file path where the .map files are located (make sure to keep the ``.\text{*}map'') and press ok.  The script outputs the .mapTimestamp files in the same folder as the .map files.  Try running the script on the example .map file provided in the ``extdata'' folder entitled ``ON\textunderscore 20140205\textunderscore MW1007-26\textunderscore DIV05\textunderscore 001.map ''

\end{description}


``Export\textunderscore timestamps\textunderscore from\textunderscore map\textunderscore files\textunderscore meadqExample.nsc''

\begin{verbatim}
doc = 0
filefilter ="C:/Users/dhall05/Desktop/*.map"
res = Dialog(doc, filefilter, "File Filter:", "string" )
% create save folder
position1 = Find(filefilter, "*" )
save_folder = Left( filefilter, position1-1)

Trace(save_folder, " timestamps have been saved in a text file")
n= GetFileCount(filefilter)

Trace(n, "files")
for i=1 to n
name = GetFileName(i)
doc = OpenDocument (name)
doc_title=GetDocTitle(doc)
length_doc_title = StrLength(doc_title)-4
doc_title=Mid(doc_title,1,length_doc_title )
save_path= save_folder + doc_title + ".mapTimestamps"

stampslabel = GetDocPath(doc) + "Timestamps"
if doc > 0
SaveAsTextFile(doc, save_path)
Trace(name, "timestamps have been saved in a text file")
CloseDocument(doc)
end
end
\end{verbatim}

\subsection{create HDF5 files from text files}


<<create-h5-files, eval = F>>=
## 
data.file <- system.file("extdata", "ON_20140205_MW1007-26_DIV07_001.mapTimestamps", package="meadq")
# user will be promted to navigate to .mapTimestamps files & log file
make.axion.map.to.h5.dh()
@





\section*{Create well summary spreadsheet}
Create two csv files \colon

\begin{description}
\item[ ont\textunderscore data\textunderscore summary\textunderscore AEfilt.csv] computes well summaries for each file using all AE (active electrodes\colon  $\geq 5$ spikes/min ) 
\item[ ont\textunderscore data\textunderscore summary\textunderscore ABEfilt.csv] computes well summaries for each file using all ABE (actively bursting electrodes\colon  $\geq 1$ burst/min)
\end{description}

<<create-well-summary, eval=FALSE >>=
data.file1 <- system.file("extdata", "ON_20140205_MW1007-26_DIV05_001.h5", package = "meadq")
data.file2 <- system.file("extdata", "ON_20140205_MW1007-26_DIV07_001.h5", package = "meadq")
data.file3 <- system.file("extdata", "ON_20140205_MW1007-26_DIV09_001.h5", package = "meadq")
h5Files = c(data.file1, data.file2, data.file3)
param.file <<- system.file( "data","chgv_parameters.rda", package = "meadq")

create_ont_csv(h5Files = h5Files, save.rdata = TRUE, param.file = param.file )
@


In the process of making the well summary, a rdata (R's native data type) has been created in the same directory where the .h5 files are located, as save.rdata argument was set to TRUE.  Rdata or .rda extention is easily loadable by R using the following commands:
<<rdata, eval=F >>=
data.file <- system.file("data", "example_ont_data.rda", package = "meadq")
load(data.file)

@





\section*{What is the ``s[[i]]'' object?}

A convention of the program is that all data referring to a recording
is stored within an object of class \texttt{mm.s}, which is actually a
list.  So, when new data/results are collected for a recording, I tend
to add the new information into that object (e.g. see how burst
analysis results are stored).

The most important items in the list are:
\begin{description}
\item[NCells] The number of units in the recording.
\item[rec.time] The start and end time of the recording.
\item[spikes] A list of vectors.  Element $i$ of the list is the
  vector of spike trains for unit $i$.  Each spike train is ordered, smallest first.
\item[nspikes] A vector.  $nspikes[i]$ is the number of spikes in
  train i.
\item[layout] Information regarding the spatial layout of the units.
\end{description}











\section*{Burst analysis}

There are several routines for burst analysis. The most common is implemented:

\begin{enumerate}
\item Max Interval method, as described by Neuroexplorer \citep{neuroexplorer}

\end{enumerate}

<<bursts-load>>=
data("example_ont_data")
@

So, for example, for electrode 2, we see the following bursts (just
taking the head as there are many of them.  We can also easily plot
the number of bursts on each electrode.

<<show-burst-info>>=
s[[3]]$channels[[2]]
head(s[[3]]$allb[[2]])
nbursts <- sapply(s[[3]]$allb, nrow)
plot(nbursts, xlab='Electrode number', ylab='Number of bursts',
     bty='n', las=1)
@


Once bursts are computed the resulting burst information can be
visualized on a raster assuming that the burst information is stored
in the \verb+s$allb+ component of the object.  Here we ask to see the
burst information for twenty seconds of data from just the first five trains.

<<burst-raster>>=
plot(s[[2]], beg=200, end=250, show.bursts=TRUE, whichcells=1:5)
@

Bursts are indicated with a red horizontal line, and the blue number
indicates the number of spikes in the burst.


Note: a Hidden-Markov Model (HMM) for burst analysis in R \citep{Tokdar2010}
is available in the following package:
\url{http://www.stat.duke.edu/~st118/Software/}.

can be used within this package, but in principle (computation time
aside as I expect an HMM to be slow) there should be no issue.  There
is also a generic ``bursts'' package:
\url{http://cran.r-project.org/web/packages/bursts/bursts.pdf}.



\section*{Plate Summary Graphics}

Get summary graphics of one of multiple files.

<<Summary-Plots, eval = FALSE >>=
h5Files = system.file("data", "example_ont_data.rda", package = "meadq")
param.file = system.file("data","chgv_parameters.rda", package = "meadq")

burst_table_Plots(param.file=param.file, h5Files=h5Files)

@





\section*{PCA: Principle components analysis}

PCA is a useful technique in the MEA data framework. Many variables or ``features'' may be derived from a spike train without knowing which among these best capture the inforation in data.  PCA takes high-dimensional data, so called for the many variables such as mean firing rate, inter burst interval, etc. comprising the data, and identifies combinations of those variables, or PC dimensions, that are responsible for the greatest percentage of variation.  In this manner, a few PC dimensions describe a large percentage of variation in the data thereby reducing the number of variables, or dimensionality, of the data.

<<PCA-start, eval=FALSE >>=
filename.data = system.file("extdata","ont_data_summary_AEfilt.csv",
                           package='meadq')
trt.params.wanted=list(DIV.wanted=c("7","9","12"), 
                       trt.wanted=c('Acetaminophen'), 
                       dose.wanted=c(1,3,10) )
ctr.params.wanted = list(DIV.wanted=c("7","9","12"), 
                         trt.wanted=c('Acetaminophen'), 
                         dose.wanted=c(0) )
output.folder = dirname( filename.data )
vars.wanted = c( "dose", "meanfiringrate","burst.per.min", "mean.isis", 
                 "per.spikes.in.burst", "mean.dur", "mean.IBIs", "nAE", 
                 "nABE", "ns.n", "ns.peak.m", "ns.durn.m" )  

PCA.by.well(filename.data = filename.data , trt.params.wanted = trt.params.wanted, 
              output.folder = output.folder, ctr.params.wanted = ctr.params.wanted,
              vars.wanted = vars.wanted )

@



<<PCA-implement, eval=TRUE, echo=FALSE >>=
filename<-system.file("examples", "subsetindividuals-dims-1-2.png", package="meadq" )

@








\section*{Network spikes: sjemea pacakge }

Network spikes are periodic elevations in activity across the whole
array \citep{Eytan2006}.  The following example shows how they are computed.
In the resulting graph, the population ``firing rate'' (the number of
active electrodes here) is shown on the y axis, time (in seconds) on
the x axis.  The horizontal red line is a threshold set for the
minimum number of active electrodes to determine a ``network spike''.
The blue dots are the peak of each network spikes.

The mean network spike is also shown, averaged across all the network
spikes in the recording.

<<example-compute.ns, eval=F>>=
example(compute.ns)
@

\section*{Correlation index: sjemea package}

The correlation index plot was devised by \citet{Wong1993} as a method to
estimate how correlation between any pair of neurons on the array
depends (if at all) upon the distance separating the pair.  For
retinal waves, the correlation index usually has an
exponentially-decaying profile.  For other recordings,
(e.g. hippocampal cultures), the profile tends to be flatter.

<<correlation-index >>=
jay.data.file <- system.file("examples", "P9_CTRL_MY1_1A.txt",
                         package = "sjemea")
jay.s <- jay.read.spikes( jay.data.file)
plot.corr.index(jay.s)
@

\subsection*{Correlation analysis: sjemea package }

We propose a new tiling-based measure for measuring the correlation
between pairs of spike trains (ongoing work by Catherine Cutts).  Here
is an example of how to compute a tiling correlation matrix for a
group of spike trains.

<<tiling-correlation>>=
data.file <- system.file("examples", "P9_CTRL_MY1_1A.txt",
                         package = "sjemea")
s <- jay.read.spikes(data.file)
t2 <- tiling.allpairwise(s)
require(lattice); levelplot(t2)
@ 



\subsection*{Acknowledgements}
Thanks to Stephen Eglen at DAMTP, Cambridge UK for his help in creating this package by building off of his package ``sjemea'', available \url{http://github.com/sje30/sjemea}.  Thanks to Tim Shafer, US EPA for his help and data.


\bibliographystyle{jneurosci}
\bibliography{sjemea}

\subsection*{Compiling this document}

<<eval=FALSE,include=TRUE>>=
require(knitr); knit2pdf('meadq-intro.Rnw')
@




\end{document}
